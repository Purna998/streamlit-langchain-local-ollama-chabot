# ğŸ¤– Streamlit-Ollama Chatbot

A full-featured chatbot application built with **Streamlit** and **Ollama** for local LLM inference. Chat with AI models running entirely on your machine!

## âœ¨ Features

- ğŸ’¬ **Interactive Chat Interface** - Clean, modern UI with message history
- ğŸ”„ **Real-time Streaming** - See responses as they're generated
- ğŸ§  **Conversation Memory** - Maintains context throughout the chat session
- ğŸ›ï¸ **Customizable Settings** - Adjust model, temperature, and max tokens
- ğŸ”’ **100% Local & Private** - All processing happens on your machine
- ğŸ¨ **Beautiful UI** - Professional design with dark mode support

## ğŸ“‹ Prerequisites

Before running this application, make sure you have:

1. **Python 3.8+** installed
2. **Ollama** installed and running
   - Download from: https://ollama.ai
   - Install at least one model (e.g., `ollama pull llama3.2`)

## ğŸš€ Installation

1. **Clone or navigate to the project directory**
   ```bash
   cd Streamlit-langchain
   ```

2. **Activate your virtual environment**
   ```bash
   .\llm-venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ¯ Usage

1. **Make sure Ollama is running**
   ```bash
   ollama serve
   ```

2. **Pull a model if you haven't already**
   ```bash
   ollama pull llama3.2
   ```

3. **Run the Streamlit app**
   ```bash
   streamlit run main.py
   ```

4. **Open your browser** to the URL shown (usually http://localhost:8501)

## ğŸ¨ Available Models

The chatbot supports various Ollama models:
- `llama3.2` - Latest Llama model (recommended)
- `llama3.1` - Previous Llama version
- `mistral` - Mistral AI model
- `codellama` - Specialized for coding tasks
- `phi3` - Microsoft's compact model

To use a model, first pull it with Ollama:
```bash
ollama pull <model-name>
```

## âš™ï¸ Configuration Options

### Temperature (0.0 - 1.0)
- **Lower values (0.0-0.3)**: More focused and deterministic responses
- **Medium values (0.4-0.7)**: Balanced creativity and coherence
- **Higher values (0.8-1.0)**: More creative and random responses

### Max Tokens
- Controls the maximum length of responses
- Range: 100 - 4096 tokens

## ğŸ› ï¸ Troubleshooting

### "Connection Error" or "Model not found"
- Ensure Ollama is running: `ollama serve`
- Verify the model is installed: `ollama list`
- Pull the model if needed: `ollama pull llama3.2`

### Slow responses
- Try a smaller model (e.g., `phi3`)
- Reduce max tokens
- Check your system resources

### Port already in use
```bash
streamlit run main.py --server.port 8502
```

## ğŸ“ Project Structure

```
Streamlit-langchain/
â”œâ”€â”€ main.py              # Main Streamlit application
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .env                 # Environment variables (optional)
â”œâ”€â”€ llm-venv/           # Virtual environment
â””â”€â”€ README.md           # This file
```

## ğŸ”§ Technical Details

- **Frontend**: Streamlit
- **LLM Integration**: LangChain with Ollama
- **Message Handling**: LangChain Core Messages
- **Session Management**: Streamlit Session State

## ğŸ“ License

This project is open source and available for personal and educational use.

## ğŸ¤ Contributing

Feel free to fork, modify, and improve this chatbot!

## ğŸ“ Support

If you encounter issues:
1. Check that Ollama is running
2. Verify your model is installed
3. Ensure all dependencies are installed
4. Check the terminal for error messages

---

**Made with â¤ï¸ using Streamlit & Ollama**
